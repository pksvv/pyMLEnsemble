{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost (Extreme Gradient Boosting)\n",
    "\n",
    "[Documentation](http://xgboost.readthedocs.io/en/latest/)\n",
    "\n",
    "[tqchen github](https://github.com/tqchen/xgboost/tree/master/demo/guide-python)\n",
    "\n",
    "[dmlc github](https://github.com/dmlc/xgboost)\n",
    "\n",
    "* “Gradient Boosting” is proposed in the paper Greedy Function Approximation: A Gradient Boosting Machine, by Friedman. \n",
    "* XGBoost is based on this original model. \n",
    "\n",
    "* Supervised Learning\n",
    "\n",
    "## Objective Function : Training Loss + Regularization\n",
    "\n",
    "$$Obj(Θ)=L(θ)+Ω(Θ)$$\n",
    "\n",
    "* $L$ is the training loss function, and \n",
    "* $Ω$ is the regularization term. \n",
    "\n",
    "### Training Loss\n",
    "\n",
    "The training loss measures how predictive our model is on training data.\n",
    "\n",
    "Example 1, Mean Squared Error for Linear Regression:\n",
    "\n",
    "$$L(θ)= \\sum_i(y_i-\\hat{y}_i)^2$$\n",
    "\n",
    "Example 2, Logistic Loss for Logistic Regression:\n",
    "\n",
    "$$ L(θ) = \\sum_i \\large[ y_i ln(1 + e^{-\\hat{y}_i}) + (1-y_i) ln(1 + e^{\\hat{y}_i}) \\large] $$\n",
    "\n",
    "### Regularization Term\n",
    "\n",
    "The regularization term controls the complexity of the model, which helps us to avoid overfitting. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[XGBoost vs GBM](https://www.quora.com/What-is-the-difference-between-the-R-gbm-gradient-boosting-machine-and-xgboost-extreme-gradient-boosting/answer/Tianqi-Chen-1)\n",
    "\n",
    "* Specifically,  xgboost used a more regularized model formalization to control over-fitting, which gives it better performance.\n",
    "\n",
    "* For model, it might be more suitable to be called as regularized gradient boosting.\n",
    "\n",
    "\n",
    "\n",
    "XGBoost (extreme Gradient Boosting) is an advanced implementation of the gradient boosting algorithm. XGBoost has proved to be a highly effective ML algorithm, extensively used in machine learning competitions and hackathons. XGBoost has high predictive power and is almost 10 times faster than the other gradient boosting techniques. It also includes a variety of regularization which reduces overfitting and improves overall performance. Hence it is also known as ‘regularized boosting‘ technique.\n",
    "\n",
    "Let us see how XGBoost is comparatively better than other techniques:\n",
    "\n",
    "* Regularization:\n",
    "  \n",
    "        Standard GBM implementation has no regularisation like XGBoost.\n",
    "        Thus XGBoost also helps to reduce overfitting.\n",
    "\n",
    "\n",
    "* Parallel Processing:\n",
    "\n",
    "        XGBoost implements parallel processing and is faster than GBM .\n",
    "        XGBoost also supports implementation on Hadoop.\n",
    "\n",
    "\n",
    "* High Flexibility:\n",
    "\n",
    "        XGBoost allows users to define custom optimization objectives and evaluation criteria adding a whole new \n",
    "        dimension to the model.\n",
    "\n",
    "\n",
    "* Handling Missing Values:\n",
    "        \n",
    "        XGBoost has an in-built routine to handle missing values.\n",
    "\n",
    "* Tree Pruning:\n",
    "        \n",
    "        XGBoost makes splits up to the max_depth specified and then starts pruning the tree backwards and removes splits \n",
    "        beyond which there is no positive gain.\n",
    "\n",
    "* Built-in Cross-Validation:\n",
    "\n",
    "        XGBoost allows a user to run a cross-validation at each iteration of the boosting process and thus it is easy to \n",
    "        get the exact optimum number of boosting iterations in a single run.\n",
    "\n",
    "**Since XGBoost takes care of the missing values itself, you do not have to impute the missing values. You can skip the step for missing value imputation from the code mentioned above. Follow the remaining steps as always and then apply xgboost as below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = sns.load_dataset('titanic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.dropna(inplace=True)\n",
    "#df['age'].fillna((df['age'].mean()),inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[['pclass', 'sex', 'age']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "lb = preprocessing.LabelBinarizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vipulgaur/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "X['sex'] = lb.fit_transform(X['sex'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df['survived']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score, cross_val_predict\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_score(clf, X_train, y_train, X_test, y_test, train=True):\n",
    "    '''\n",
    "    print the accuracy score, classification report and confusion matrix of classifier\n",
    "    '''\n",
    "    if train:\n",
    "        '''\n",
    "        training performance\n",
    "        '''\n",
    "        print(\"Train Result:\\n\")\n",
    "        print(\"accuracy score: {0:.4f}\\n\".format(accuracy_score(y_train, clf.predict(X_train))))\n",
    "        print(\"Classification Report: \\n {}\\n\".format(classification_report(y_train, clf.predict(X_train))))\n",
    "        print(\"Confusion Matrix: \\n {}\\n\".format(confusion_matrix(y_train, clf.predict(X_train))))\n",
    "\n",
    "        res = cross_val_score(clf, X_train, y_train, cv=10, scoring='accuracy')\n",
    "        print(\"Average Accuracy: \\t {0:.4f}\".format(np.mean(res)))\n",
    "        print(\"Accuracy SD: \\t\\t {0:.4f}\".format(np.std(res)))\n",
    "        \n",
    "    elif train==False:\n",
    "        '''\n",
    "        test performance\n",
    "        '''\n",
    "        print(\"Test Result:\\n\")        \n",
    "        print(\"accuracy score: {0:.4f}\\n\".format(accuracy_score(y_test, clf.predict(X_test))))\n",
    "        print(\"Classification Report: \\n {}\\n\".format(classification_report(y_test, clf.predict(X_test))))\n",
    "        print(\"Confusion Matrix: \\n {}\\n\".format(confusion_matrix(y_test, clf.predict(X_test))))    \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "!pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_clf = xgb.XGBClassifier(max_depth=5, n_estimators=10000, learning_rate=0.3,\n",
    "                            n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=1, gamma=0,\n",
       "              learning_rate=0.3, max_delta_step=0, max_depth=5,\n",
       "              min_child_weight=1, missing=None, n_estimators=10000, n_jobs=-1,\n",
       "              nthread=None, objective='binary:logistic', random_state=0,\n",
       "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
       "              silent=None, subsample=1, verbosity=1)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Result:\n",
      "\n",
      "accuracy score: 0.8925\n",
      "\n",
      "Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.93      0.91       382\n",
      "           1       0.88      0.83      0.86       241\n",
      "\n",
      "    accuracy                           0.89       623\n",
      "   macro avg       0.89      0.88      0.89       623\n",
      "weighted avg       0.89      0.89      0.89       623\n",
      "\n",
      "\n",
      "Confusion Matrix: \n",
      " [[355  27]\n",
      " [ 40 201]]\n",
      "\n",
      "Average Accuracy: \t 0.8073\n",
      "Accuracy SD: \t\t 0.0293\n"
     ]
    }
   ],
   "source": [
    "print_score(xgb_clf, X_train, y_train, X_test, y_test, train=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Result:\n",
      "\n",
      "accuracy score: 0.7985\n",
      "\n",
      "Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.87      0.84       167\n",
      "           1       0.76      0.67      0.72       101\n",
      "\n",
      "    accuracy                           0.80       268\n",
      "   macro avg       0.79      0.77      0.78       268\n",
      "weighted avg       0.80      0.80      0.80       268\n",
      "\n",
      "\n",
      "Confusion Matrix: \n",
      " [[146  21]\n",
      " [ 33  68]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_score(xgb_clf, X_train, y_train, X_test, y_test, train=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Classifier | Decision Tree | Bagging | Random Forest | Optimised RF | Extra-Trees | AdaBoost (CART) | AdaBoost (RF) | Gradient Boosting |\n",
    "|:-|:-|:- |:- |:- |:- |:-|:-| :- |\n",
    "| Train accuracy score | 0.9528 | 0.9528 | 0.9325 | 0.9264 | 0.9448 | 0.8661 | 0.9528 | 0.9449 |\n",
    "| Average accuracy score | 0.7724 | 0.7879 | 0.7801 | 0.7059 | 0.7548 | 0.7793 | 0.7353 | 0.7906 |\n",
    "| SD | 0.1018 | 0.1008 | 0.1474 | 0.1308 | 0.1406 | 0.1172 | 0.0881 | 0.0912 |\n",
    "| Test accuracy score | 0.7636 | 0.7455 | 0.7895 | 0.6316 | 0.7895 | 0.6545 | 0.7818 | 0.7818 |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise\n",
    "\n",
    "# Since XGB doesn't require any missing value imputation, run the last section with whole dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
